ğŸ ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ í”„ë¡œë•ì…˜ ë§ˆìŠ¤í„° ë¡œë“œë§µ

ì™„ë²½í•œ ëª©í‘œ ì„¤ì •ì´ë„¤ìš”! ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ í”„ë¡œë•ì…˜ í™˜ê²½ì„ ìœ„í•œ ì²´ê³„ì ì¸ í•™ìŠµ ê³„íšì„ ì„¸ì›Œë“œë¦¬ê² ìŠµë‹ˆë‹¤.

ğŸ“ˆ **1ë‹¨ê³„: Python ê¸°ì´ˆ ê°•í™” (2ì£¼)**

### í•™ìŠµ ëª©í‘œ
- í´ë˜ìŠ¤, ë°ì½”ë ˆì´í„°, íŒŒì¼ ì…ì¶œë ¥, ì˜ˆì™¸ ì²˜ë¦¬ ì™„ì „ ì •ë³µ
- ë°ì´í„° ì²˜ë¦¬ì— ìµœì í™”ëœ Python ì½”ë“œ ì‘ì„±

### ì‹¤ë¬´ í”„ë¡œì íŠ¸: ë°ì´í„° ê²€ì¦ ì‹œìŠ¤í…œ
```python
# ëª©í‘œ: ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ë¼ì´ë¸ŒëŸ¬ë¦¬ êµ¬í˜„
class DataValidator:
    def __init__(self, config_path: str):
        # TODO: ì„¤ì • íŒŒì¼ ì½ê¸°
        pass
    
    @property
    def validation_rules(self):
        # TODO: ê²€ì¦ ê·œì¹™ ë°˜í™˜
        pass
    
    def validate_dataset(self, df: pd.DataFrame) -> Dict[str, Any]:
        # TODO: ë°ì´í„°ì…‹ ê²€ì¦ ë° ë¦¬í¬íŠ¸ ìƒì„±
        pass

# ë°ì½”ë ˆì´í„° ì‹¤ìŠµ
def performance_monitor(func):
    # TODO: í•¨ìˆ˜ ì‹¤í–‰ ì‹œê°„ ë° ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
    pass

@performance_monitor
def process_large_dataset(file_path: str):
    # TODO: ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ í•¨ìˆ˜
    pass
```

### í‰ê°€ ê¸°ì¤€
- [ ] í´ë˜ìŠ¤ ìƒì†ê³¼ ë‹¤í˜•ì„± í™œìš©
- [ ] ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €ë¡œ íŒŒì¼ ì•ˆì „ ì²˜ë¦¬
- [ ] ì»¤ìŠ¤í…€ ì˜ˆì™¸ í´ë˜ìŠ¤ êµ¬í˜„
- [ ] ì„±ëŠ¥ ì¸¡ì • ë°ì½”ë ˆì´í„° êµ¬í˜„

---

## ğŸ“Š **2ë‹¨ê³„: ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ (3ì£¼)**

### í•™ìŠµ ëª©í‘œ
- Pandas ê³ ê¸‰ ê¸°ëŠ¥ (ë©€í‹°ì¸ë±ìŠ¤, ê·¸ë£¹ë°”ì´, í”¼ë²—)
- Daskë¥¼ í™œìš©í•œ ë³‘ë ¬ ì²˜ë¦¬
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë°ì´í„° ì²˜ë¦¬ íŒ¨í„´

### ì‹¤ë¬´ í”„ë¡œì íŠ¸: ì‹¤ì‹œê°„ ë¡œê·¸ ë¶„ì„ ì‹œìŠ¤í…œ
```python
# ìš”êµ¬ì‚¬í•­
# 1. 10GB+ ë¡œê·¸ íŒŒì¼ì„ ì²­í¬ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
# 2. ì‹¤ì‹œê°„ í†µê³„ ì§‘ê³„ (ë¶„ë‹¹ ìš”ì²­ ìˆ˜, ì—ëŸ¬ìœ¨ ë“±)
# 3. ì´ìƒ íŒ¨í„´ íƒì§€ ì•Œê³ ë¦¬ì¦˜
# 4. ê²°ê³¼ë¥¼ Parquet í¬ë§·ìœ¼ë¡œ ì €ì¥
# 5. Daskë¡œ ë¶„ì‚° ì²˜ë¦¬

import dask.dataframe as dd
from typing import Iterator, Tuple

class LogAnalyzer:
    def __init__(self, chunk_size: int = 10000):
        self.chunk_size = chunk_size
        
    def process_logs_stream(self, log_path: str) -> Iterator[pd.DataFrame]:
        # TODO: ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ë¡œê·¸ ì²˜ë¦¬
        pass
        
    def detect_anomalies(self, df: dd.DataFrame) -> dd.DataFrame:
        # TODO: ì´ìƒ íŒ¨í„´ íƒì§€
        pass
```

### í‰ê°€ ê¸°ì¤€
- [ ] ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 1GB ì´í•˜ë¡œ 10GB íŒŒì¼ ì²˜ë¦¬
- [ ] Dask í´ëŸ¬ìŠ¤í„°ë¡œ ì²˜ë¦¬ ì‹œê°„ 50% ë‹¨ì¶•
- [ ] ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ êµ¬í˜„
- [ ] ê²°ê³¼ ì •í™•ë„ 95% ì´ìƒ

---

## ğŸ¤– **3ë‹¨ê³„: ML ëª¨ë¸ ê°œë°œ & ê´€ë¦¬ (3ì£¼)**

### í•™ìŠµ ëª©í‘œ
- MLflowë¡œ ì‹¤í—˜ ê´€ë¦¬ ë° ëª¨ë¸ ë²„ì €ë‹
- ëª¨ë¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ
- A/B í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬

### ì‹¤ë¬´ í”„ë¡œì íŠ¸: ì¶”ì²œ ì‹œìŠ¤í…œ MLOps íŒŒì´í”„ë¼ì¸
```python
# ì•„í‚¤í…ì²˜
# 1. ë°ì´í„° ìˆ˜ì§‘ â†’ ì „ì²˜ë¦¬ â†’ ëª¨ë¸ í•™ìŠµ â†’ í‰ê°€ â†’ ë°°í¬
# 2. MLflowë¡œ ì‹¤í—˜ ì¶”ì 
# 3. ëª¨ë¸ ì„±ëŠ¥ ë“œë¦¬í”„íŠ¸ ê°ì§€
# 4. ìë™ ì¬í•™ìŠµ íŠ¸ë¦¬ê±°

import mlflow
import mlflow.sklearn
from mlflow.tracking import MlflowClient

class RecommendationMLOps:
    def __init__(self, experiment_name: str):
        mlflow.set_experiment(experiment_name)
        self.client = MlflowClient()
        
    def train_model(self, X_train, y_train, hyperparams: Dict):
        with mlflow.start_run():
            # TODO: ëª¨ë¸ í•™ìŠµ ë° MLflow ë¡œê¹…
            pass
            
    def deploy_best_model(self, metric: str = "accuracy"):
        # TODO: ìµœê³  ì„±ëŠ¥ ëª¨ë¸ í”„ë¡œë•ì…˜ ë°°í¬
        pass
        
    def monitor_model_performance(self):
        # TODO: ëª¨ë¸ ë“œë¦¬í”„íŠ¸ ê°ì§€
        pass
```

### í‰ê°€ ê¸°ì¤€
- [ ] MLflow UIì—ì„œ ì‹¤í—˜ ê²°ê³¼ ì‹œê°í™”
- [ ] ëª¨ë¸ ë²„ì „ ê´€ë¦¬ ë° ë¡¤ë°± ê¸°ëŠ¥
- [ ] ì„±ëŠ¥ ë“œë¦¬í”„íŠ¸ ìë™ ê°ì§€
- [ ] Docker ì»¨í…Œì´ë„ˆë¡œ ëª¨ë¸ ì„œë¹™

---

## ğŸ”„ **4ë‹¨ê³„: ë°ì´í„° íŒŒì´í”„ë¼ì¸ êµ¬ì¶• (4ì£¼)**

### í•™ìŠµ ëª©í‘œ
- Airflow DAG ì„¤ê³„ ë° ìŠ¤ì¼€ì¤„ë§
- Prefect í´ë¼ìš°ë“œ ì›Œí¬í”Œë¡œìš°
- íŒŒì´í”„ë¼ì¸ ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼

### ì‹¤ë¬´ í”„ë¡œì íŠ¸: E-commerce ë°ì´í„° íŒŒì´í”„ë¼ì¸
```python
# ìš”êµ¬ì‚¬í•­
# 1. ë§¤ì¼ ì˜¤ì „ 2ì‹œ ë°ì´í„° ìˆ˜ì§‘
# 2. ETL íŒŒì´í”„ë¼ì¸ (ì¶”ì¶œâ†’ë³€í™˜â†’ì ì¬)
# 3. ë°ì´í„° í’ˆì§ˆ ê²€ì¦
# 4. ì‹¤íŒ¨ ì‹œ Slack ì•Œë¦¼
# 5. ì¬ì²˜ë¦¬ ìë™í™”

from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta

def create_etl_dag():
    default_args = {
        'owner': 'data-team',
        'depends_on_past': False,
        'start_date': datetime(2024, 1, 1),
        'email_on_failure': True,
        'retries': 3,
        'retry_delay': timedelta(minutes=5)
    }
    
    dag = DAG(
        'ecommerce_etl',
        default_args=default_args,
        description='Daily ETL pipeline',
        schedule_interval='0 2 * * *',
        catchup=False
    )
    
    # TODO: íƒœìŠ¤í¬ ì •ì˜
    return dag

# Prefect ë²„ì „
from prefect import flow, task
from prefect.blocks.system import Secret

@task
def extract_data(source: str) -> pd.DataFrame:
    # TODO: ë°ì´í„° ì¶”ì¶œ
    pass

@task
def transform_data(df: pd.DataFrame) -> pd.DataFrame:
    # TODO: ë°ì´í„° ë³€í™˜
    pass

@flow
def etl_pipeline():
    # TODO: ETL í”Œë¡œìš° êµ¬ì„±
    pass
```

### í‰ê°€ ê¸°ì¤€
- [ ] 99.9% íŒŒì´í”„ë¼ì¸ ì„±ê³µë¥ 
- [ ] í‰ê·  ì²˜ë¦¬ ì‹œê°„ 1ì‹œê°„ ì´í•˜
- [ ] ì‹¤íŒ¨ ë³µêµ¬ ìë™í™”
- [ ] ë°ì´í„° í’ˆì§ˆ SLA ì¤€ìˆ˜

---

## ğŸš€ **5ë‹¨ê³„: API ì„œë¹™ & ë°°í¬ (3ì£¼)**

### í•™ìŠµ ëª©í‘œ
- FastAPIë¡œ ê³ ì„±ëŠ¥ ML API êµ¬ì¶•
- ë¹„ë™ê¸° ì²˜ë¦¬ ë° ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…
- ë¡œë“œ ë°¸ëŸ°ì‹± ë° ì˜¤í†  ìŠ¤ì¼€ì¼ë§

### ìµœì¢… í”„ë¡œì íŠ¸: ì‹¤ì‹œê°„ ì˜ˆì¸¡ API ì„œë¹„ìŠ¤
```python
# ì•„í‚¤í…ì²˜
# Frontend â†’ API Gateway â†’ FastAPI â†’ ML Model â†’ Database
# ìš”êµ¬ì‚¬í•­: ë™ì‹œ ìš”ì²­ 1000ê°œ, ì‘ë‹µì‹œê°„ 100ms ì´í•˜

from fastapi import FastAPI, BackgroundTasks, HTTPException
from pydantic import BaseModel
import asyncio
import aioredis
from typing import List, Optional

app = FastAPI(title="ML Prediction API", version="1.0.0")

class PredictionRequest(BaseModel):
    features: List[float]
    model_version: Optional[str] = "latest"

class PredictionResponse(BaseModel):
    prediction: float
    confidence: float
    model_version: str
    processing_time_ms: float

@app.post("/predict", response_model=PredictionResponse)
async def predict(request: PredictionRequest, background_tasks: BackgroundTasks):
    # TODO: ë¹„ë™ê¸° ì˜ˆì¸¡ ì²˜ë¦¬
    pass

@app.get("/health")
async def health_check():
    # TODO: í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸
    pass

# ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…
@app.post("/batch-predict")
async def batch_predict(file_path: str, background_tasks: BackgroundTasks):
    background_tasks.add_task(process_batch_predictions, file_path)
    return {"message": "Batch processing started"}

async def process_batch_predictions(file_path: str):
    # TODO: ë°°ì¹˜ ì˜ˆì¸¡ ì²˜ë¦¬
    pass
```

### Docker & Kubernetes ë°°í¬
```dockerfile
# Dockerfile
FROM python:3.11-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
EXPOSE 8000
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

```yaml
# k8s-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ml-api
spec:
  replicas: 3
  selector:
    matchLabels:
      app: ml-api
  template:
    metadata:
      labels:
        app: ml-api
    spec:
      containers:
      - name: ml-api
        image: ml-api:latest
        ports:
        - containerPort: 8000
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
```

### ìµœì¢… í‰ê°€ ê¸°ì¤€
- [ ] API ì‘ë‹µ ì‹œê°„ 100ms ì´í•˜
- [ ] ë™ì‹œ ìš”ì²­ 1000ê°œ ì²˜ë¦¬ ê°€ëŠ¥
- [ ] 99.99% ê°€ìš©ì„± ë‹¬ì„±
- [ ] ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ êµ¬ì¶•
- [ ] CI/CD íŒŒì´í”„ë¼ì¸ ì™„ì„±

---

## ğŸ¯ **í•™ìŠµ ì²´í¬í¬ì¸íŠ¸**

### ì£¼ê°„ ì ê²€ í•­ëª©
1. **Week 2**: Python ê¸°ì´ˆ í”„ë¡œì íŠ¸ ì™„ì„±ë„ 80%
2. **Week 5**: 10GB ë°ì´í„° ì²˜ë¦¬ ì„±ê³µ
3. **Week 8**: MLflow ì‹¤í—˜ 10ê°œ ì´ìƒ ì™„ë£Œ
4. **Week 12**: Airflow DAG ì •ìƒ ìš´ì˜
5. **Week 15**: API ì„œë¹„ìŠ¤ í”„ë¡œë•ì…˜ ë°°í¬

### ìµœì¢… í¬íŠ¸í´ë¦¬ì˜¤
- GitHub ì €ì¥ì†Œ (ì½”ë“œ + ë¬¸ì„œ)
- ë¼ì´ë¸Œ ë°ëª¨ ì‹œìŠ¤í…œ
- ê¸°ìˆ  ë¸”ë¡œê·¸ í¬ìŠ¤íŒ… 5í¸
- ì•„í‚¤í…ì²˜ ì„¤ê³„ ë¬¸ì„œ

---

## ğŸ”¥ **ì²« ë²ˆì§¸ ì‹¤ë ¥ ì§„ë‹¨ í…ŒìŠ¤íŠ¸**

### ë¬¸ì œ 1: í´ë˜ìŠ¤ì™€ ë°ì½”ë ˆì´í„° (30ë¶„)
```python
# ë‹¤ìŒ ìš”êµ¬ì‚¬í•­ì„ ë§Œì¡±í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”:
# 1. CSV íŒŒì¼ì„ ì½ì–´ì„œ ë°ì´í„° ê²€ì¦í•˜ëŠ” í´ë˜ìŠ¤
# 2. ì‹¤í–‰ ì‹œê°„ì„ ì¸¡ì •í•˜ëŠ” ë°ì½”ë ˆì´í„°
# 3. ì»¤ìŠ¤í…€ ì˜ˆì™¸ ì²˜ë¦¬

class DataValidationError(Exception):
    pass

def timing_decorator(func):
    # TODO: êµ¬í˜„
    pass

class CSVProcessor:
    def __init__(self, file_path: str):
        # TODO: êµ¬í˜„
        pass
    
    @timing_decorator
    def validate_data(self):
        # TODO: ë°ì´í„° ê²€ì¦ ë¡œì§
        # ë¹ˆ ê°’, íƒ€ì… ì˜¤ë¥˜ ë“± ì²´í¬
        pass
```

### ë¬¸ì œ 2: ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ (45ë¶„)
```python
# 10GB ë¡œê·¸ íŒŒì¼ì„ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ì„¸ìš”
# - ì²­í¬ ë‹¨ìœ„ë¡œ ì½ê¸°
# - ì—ëŸ¬ ë¡œê·¸ë§Œ í•„í„°ë§
# - ì‹œê°„ëŒ€ë³„ í†µê³„ ìƒì„±

def process_large_log(file_path: str, chunk_size: int = 10000):
    # TODO: êµ¬í˜„
    pass
```

### ë¬¸ì œ 3: ë¹„ë™ê¸° API (30ë¶„)
```python
# FastAPIë¡œ ê°„ë‹¨í•œ ì˜ˆì¸¡ API ë§Œë“¤ê¸°
from fastapi import FastAPI

app = FastAPI()

# TODO: POST /predict ì—”ë“œí¬ì¸íŠ¸ êµ¬í˜„
# - ì…ë ¥: {"data": [1, 2, 3, 4, 5]}
# - ì¶œë ¥: {"prediction": float, "timestamp": str}
```

**ì§€ê¸ˆ ë°”ë¡œ 1ë‹¨ê³„ í”„ë¡œì íŠ¸ë¶€í„° ì‹œì‘í•´ë³¼ê¹Œìš”?** ğŸš€